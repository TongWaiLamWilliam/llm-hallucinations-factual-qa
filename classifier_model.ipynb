{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ba918-af3a-43b1-8980-5b522e5fbf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu = \"0\"\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9174d45-6cdc-4f31-8b48-ef21497072c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_results = list(Path(\"/home/ec2-user/SageMaker/halu_code/results/\").rglob(\"*.pickle\"))\n",
    "print (inference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e9ba4-28e9-411c-aa9b-46667101c23c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_capitals(response, answer):\n",
    "    if \",\" in answer:\n",
    "        answer = answer.split(',')[0]\n",
    "    return answer in response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805fd5d-8502-465b-8397-b3ccff3ff591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFHallucinationClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_shape, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #https://arxiv.org/pdf/2304.13734.pdf\n",
    "        self.linear_relu_stack =torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_shape, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.torch.nn.Linear(256, 128),\n",
    "            torch.torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.Linear(64, 2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "class RNNHallucinationClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = 128\n",
    "        num_layers = 4\n",
    "        self.lstm = torch.nn.GRU(1, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=False)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, 2)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        lstm_out, _ = self.lstm(seq)\n",
    "        return self.linear(lstm_out)[-1, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ea63d-8be1-4e81-a021-eaff7991641b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_classifier_roc(inputs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, correct.astype(int), test_size = 0.2)\n",
    "    classifier_model = FFHallucinationClassifier(X_train.shape[1]).to(device)\n",
    "    X_train = torch.tensor(X_train).to(device)\n",
    "    y_train = torch.tensor(y_train).to(torch.long).to(device)\n",
    "    X_test = torch.tensor(X_test).to(device)\n",
    "    y_test = torch.tensor(y_test).to(torch.long).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(classifier_model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "\n",
    "    for i in range(1001):\n",
    "        optimizer.zero_grad()\n",
    "        sample = torch.randperm(X_train.shape[0])[:512]\n",
    "        pred = classifier_model(X_train[sample])\n",
    "        loss = torch.nn.functional.cross_entropy(pred, y_train[sample])\n",
    "        loss.backward()\n",
    "        #lr_scheduler.step()\n",
    "        optimizer.step()\n",
    "    classifier_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = torch.nn.functional.softmax(classifier_model(X_test), dim=1)\n",
    "        prediction_classes = (pred[:,1]>0.5).type(torch.long).cpu()\n",
    "        return roc_auc_score(y_test.cpu(), pred[:,1].cpu()), (prediction_classes.numpy()==y_test.cpu().numpy()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cf47c-b4fd-46d7-8fbd-d00f1792dadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a09e0-6a85-4778-8adf-807b8d73c94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, results_file in enumerate(tqdm(inference_results)):\n",
    "    if results_file not in all_results.keys():\n",
    "        try:\n",
    "            del results\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            classifier_results = {}\n",
    "            with open(results_file, \"rb\") as infile:\n",
    "                results = pickle.loads(infile.read())\n",
    "            correct = np.array(results['correct'])\n",
    "            if 'capitals' in results_file.stem:\n",
    "                correct = np.array([check_capitals(i, j) for i,j in zip(results['str_response'], results['answers'])])\n",
    "    \n",
    "            # attributes\n",
    "            X_train, X_test, y_train, y_test = train_test_split(results['attributes_first'], correct.astype(int), test_size = 0.2)\n",
    "            batch_size = 512\n",
    "            #batch_size = 4\n",
    "\n",
    "            rnn_model = RNNHallucinationClassifier()\n",
    "            optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "\n",
    "            for step in range(10):\n",
    "                x_sub, y_sub = zip(*random.sample(list(zip(X_train, y_train)), batch_size))\n",
    "                y_sub = torch.tensor(y_sub).to(torch.long)\n",
    "                optimizer.zero_grad()\n",
    "                preds = torch.stack([rnn_model(torch.tensor(i).view(1, -1, 1).to(torch.float)) for i in x_sub])\n",
    "                loss = torch.nn.functional.cross_entropy(preds, y_sub)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            preds = torch.stack([rnn_model(torch.tensor(i).view(1, -1, 1).to(torch.float)) for i in X_test])\n",
    "            preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "            prediction_classes = (preds[:,1]>0.5).type(torch.long).cpu()\n",
    "            #print(roc_auc_score(y_test, preds[:,1].detach().cpu().numpy()))\n",
    "            classifier_results['attribution_rnn_roc'] = roc_auc_score(y_test, preds[:,1].detach().cpu().numpy())\n",
    "            classifier_results['attribution_rnn_acc'] = (prediction_classes.numpy()==y_test).mean()\n",
    "\n",
    "            # logits\n",
    "            first_logits = np.stack([sp.special.softmax(i[j]) for i,j in zip(results['logits'], results['start_pos'])])\n",
    "            first_logits_roc, first_logits_acc = gen_classifier_roc(first_logits)\n",
    "            #print(first_logits_roc)\n",
    "            classifier_results['first_logits_roc'] = first_logits_roc\n",
    "            classifier_results['first_logits_acc'] = first_logits_acc\n",
    "\n",
    "            final_logits = np.stack([sp.special.softmax(i[-1]) for i in results['logits']])\n",
    "            final_logits_roc, final_logits_acc = gen_classifier_roc(first_logits)\n",
    "            #print(final_logits_roc)\n",
    "            classifier_results['final_logits_roc'] = final_logits_roc\n",
    "            classifier_results['final_logits_acc'] = final_logits_acc\n",
    "\n",
    "            # fully connected\n",
    "            for layer in range(results['first_fully_connected'][0].shape[0]):\n",
    "                layer_roc, layer_acc = gen_classifier_roc(np.stack([i[layer] for i in results['first_fully_connected']]))\n",
    "                classifier_results[f'first_fully_connected_roc_{layer}'] = layer_roc\n",
    "                classifier_results[f'first_fully_connected_acc_{layer}'] = layer_acc\n",
    "\n",
    "            for layer in range(results['final_fully_connected'][0].shape[0]):\n",
    "                layer_roc, layer_acc = gen_classifier_roc(np.stack([i[layer] for i in results['final_fully_connected']]))\n",
    "                classifier_results[f'final_fully_connected_roc_{layer}'] = layer_roc\n",
    "                classifier_results[f'final_fully_connected_acc_{layer}'] = layer_acc\n",
    "\n",
    "            # attention\n",
    "            for layer in range(results['first_attention'][0].shape[0]):\n",
    "                layer_roc, layer_acc = gen_classifier_roc(np.stack([i[layer] for i in results['first_attention']]))\n",
    "                classifier_results[f'first_attention_roc_{layer}'] = layer_roc\n",
    "                classifier_results[f'first_attention_acc_{layer}'] = layer_acc\n",
    "\n",
    "            for layer in range(results['final_attention'][0].shape[0]):\n",
    "                layer_roc, layer_acc = gen_classifier_roc(np.stack([i[layer] for i in results['final_attention']]))\n",
    "                classifier_results[f'final_attention_roc_{layer}'] = layer_roc\n",
    "                classifier_results[f'final_attention_acc_{layer}'] = layer_acc\n",
    "            \n",
    "            all_results[results_file] = classifier_results.copy()\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dd8b9-8a1a-4737-8cad-248c1ec4ac54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(all_results.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hallucination",
   "language": "python",
   "name": "conda_hallucination"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
