{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556a861-1ff9-45ab-ab0c-b869c9c50d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statistics\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62dfa62-5a05-45d2-b15e-fdcd66d1c862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu = \"0\"\n",
    "org=\"tiiuae\"\n",
    "model_name = \"falcon-7b\"\n",
    "repo = f\"{org}/{model_name}\"\n",
    "dataset_name = \"trivia\"\n",
    "num_samples = 5\n",
    "start = int(gpu) * num_samples\n",
    "end = start + num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cbdaf-5e7f-4f12-838f-52365e1220d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392d8a9-c11c-4df4-8992-f88a4c86a58e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, cache_dir=\"/home/ec2-user/SageMaker/halu_code/cache/data\", \n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338286c3-6406-417a-a306-16a9272b7928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)\n",
    "selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc.\n",
    "\n",
    "if dataset_name in [\"capitals\", \"place_of_birth\", \"founders\"]:\n",
    "    pd_frame = pd.read_csv(f'/home/ec2-user/SageMaker/halu_code/data/{dataset_name}.csv')\n",
    "    dataset = [(pd_frame.iloc[i]['subject'], pd_frame.iloc[i]['target']) for i in range(start, end)]\n",
    "elif dataset_name==\"trivia\":\n",
    "    trivia_qa = load_dataset('trivia_qa', 'rc.nocontext', cache_dir='/home/ec2-user/SageMaker/halu_code/cache/data')\n",
    "    full_dataset = []\n",
    "    for obs in tqdm(trivia_qa['train']):\n",
    "        aliases = []\n",
    "        aliases.extend(obs['answer']['aliases'])\n",
    "        aliases.extend(obs['answer']['normalized_aliases'])\n",
    "        aliases.append(obs['answer']['value'])\n",
    "        aliases.append(obs['answer']['normalized_value'])\n",
    "        full_dataset.append((obs['question'], aliases))\n",
    "    dataset = full_dataset[start: end]\n",
    "print (\"Loaded training data\")\n",
    "    \n",
    "num_samples_per_gpu = 10 #1000\n",
    "start_pos = int(gpu) * num_samples_per_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92d8fe-0427-4c1d-a8b8-74fee3758f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_next_token(x):\n",
    "    with torch.no_grad():\n",
    "        return model(x).logits\n",
    "    \n",
    "def generate_response(x, max_length=100, pbar=False):\n",
    "    response = []\n",
    "    bar = tqdm(range(max_length)) if pbar else range(max_length)\n",
    "    for step in bar:\n",
    "        logits = get_next_token(x)\n",
    "        next_token = logits.squeeze()[-1].argmax()\n",
    "        x = torch.concat([x, next_token.view(1, -1)], dim=1)\n",
    "        response.append(next_token)\n",
    "        if next_token == tokenizer.encode(str(tokenizer._eos_token))[0] and step>5:\n",
    "            break\n",
    "    return torch.stack(response).cpu().numpy(), logits.squeeze()\n",
    "\n",
    "def answer_question(question, tokenizer, max_length=100, pbar=False):\n",
    "    input_ids = tokenizer(question, return_tensors='pt').input_ids.to(device)\n",
    "    response, logits = generate_response(input_ids, max_length=max_length, pbar=pbar)\n",
    "    return response, logits, input_ids.shape[-1]\n",
    "\n",
    "def generate_responses(question, str_response, tokenizer, temperature, n_trials=3):\n",
    "    # generate 3 responses to the question and (self)check them against the zero temp response\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    start_pos = inputs.size(dim=-1)\n",
    "    \n",
    "    assert n_trials > 1\n",
    "    \n",
    "    hitemp_str_responses = []\n",
    "    \n",
    "    for i in range (0, n_trials):\n",
    "        model_outputs = model.generate(inputs, do_sample=True, temperature=temperature, max_new_tokens=100, return_dict_in_generate=True, output_scores=True)\n",
    "        generated_tokens_ids = model_outputs.sequences[0]\n",
    "        response = tokenizer.decode(generated_tokens_ids[start_pos:]).replace(\"\\n\", \" \").strip()\n",
    "        hitemp_str_responses.append(response)\n",
    "        \n",
    "    selfcheck_scores_bert_overall = []\n",
    "    selfcheck_scores_bert_average = []\n",
    "    selfcheck_ngram_overall = []\n",
    "    \n",
    "    sentences = [str_response]\n",
    "    overall_bertscore = selfcheck_bertscore.predict(\n",
    "        sentences = sentences,                          # list of sentences\n",
    "        sampled_passages = hitemp_str_responses, # list of sampled passages\n",
    "        )\n",
    "    #print(overall_bertscore)\n",
    "    selfcheck_scores_bert_overall.append(overall_bertscore[0])\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    sentences = [sent for sent in nlp(str_response).sents]\n",
    "    sentences = [sent.text.strip() for sent in sentences if len(sent) > 3]\n",
    "    all_bertscores = selfcheck_bertscore.predict(\n",
    "        sentences = sentences,                          # list of sentences\n",
    "        sampled_passages = hitemp_str_responses, # list of sampled passages\n",
    "        )\n",
    "    #print(all_bertscores)\n",
    "    average_bertscore = statistics.mean(all_bertscores)\n",
    "    selfcheck_scores_bert_average.append(average_bertscore)\n",
    "      \n",
    "    \n",
    "    sent_scores_ngram = selfcheck_ngram.predict(\n",
    "        sentences = sentences,   \n",
    "        passage = str_response,\n",
    "        sampled_passages = hitemp_str_responses,\n",
    "    )\n",
    "    #print(sent_scores_ngram)\n",
    "    selfcheck_ngram_overall.append(sent_scores_ngram)\n",
    "    \n",
    "          \n",
    "    return hitemp_str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall\n",
    "\n",
    "def answer_trivia(question, targets, tokenizer, temperature):\n",
    "    response, logits, start_pos = answer_question(question, tokenizer)\n",
    "    str_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall =\\\n",
    "            generate_responses(question, str_response, tokenizer, temperature)\n",
    "    correct = False\n",
    "    for alias in targets:\n",
    "        if alias.lower() in str_response.lower():\n",
    "            correct = True\n",
    "            break\n",
    "    return response, str_response, logits, start_pos, correct,\\\n",
    "            str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall\n",
    "\n",
    "def answer_capitals(source, target, tokenizer, temperature):\n",
    "    question = f\"What is the capital of {source}?\"\n",
    "    response, logits, start_pos = answer_question(question, tokenizer)\n",
    "    str_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall =\\\n",
    "            generate_responses(question, str_response, tokenizer, temperature)\n",
    "    correct = False\n",
    "    if target.lower() in str_response.lower():\n",
    "        correct = True\n",
    "    return response, str_response, logits, start_pos, correct,\\\n",
    "            str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall\n",
    "\n",
    "def answer_birth_place(source, target, tokenizer, temperature):\n",
    "    question = f\"Where was {source} born?\"\n",
    "    response, logits, start_pos = answer_question(question, tokenizer)\n",
    "    str_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall =\\\n",
    "            generate_responses(question, str_response, tokenizer, temperature)\n",
    "    correct = False\n",
    "    if target.lower() in str_response.lower():\n",
    "        correct = True\n",
    "    return response, str_response, logits, start_pos, correct,\\\n",
    "            str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall\n",
    "\n",
    "def answer_founders(source, target, tokenizer, temperature):\n",
    "    question = f\"Who founded {source}?\"\n",
    "    response, logits, start_pos = answer_question(question, tokenizer)\n",
    "    str_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall =\\\n",
    "            generate_responses(question, str_response, tokenizer, temperature)\n",
    "    correct = False\n",
    "    if target.lower() in str_response.lower():\n",
    "        correct = True\n",
    "    return response, str_response, logits, start_pos, correct,\\\n",
    "            str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba328f-3eb7-4703-aab2-470ccb3cdcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temperature = 1.0\n",
    "    \n",
    "selfcheck_dict = {\n",
    "        'question': [],\n",
    "        'response': [],\n",
    "        'str_response': [],\n",
    "        'start_pos': [],\n",
    "        'correct': [],\n",
    "        'hitemp_str_responses': [],\n",
    "        'selfcheck_scores_bert_overall': [],\n",
    "        'selfcheck_scores_bert_average': [],\n",
    "        'selfcheck_ngram_overall': []\n",
    "    }\n",
    "\n",
    "selfcheck_arr_overall = []\n",
    "selfcheck_arr_average = []\n",
    "selfcheck_ngram_average = []\n",
    "correct_arr = []\n",
    "\n",
    "if dataset_name==\"trivia\":\n",
    "    answer_func = answer_trivia\n",
    "elif dataset_name==\"capitals\":\n",
    "    answer_func = answer_capitals\n",
    "elif dataset_name==\"place_of_birth\":\n",
    "    answer_func = answer_birth_place\n",
    "elif dataset_name==\"founders\":\n",
    "    answer_func = answer_birth_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00f8f2-51fa-4af7-8d50-7e5cac40c6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in tqdm(range(start, end)):\n",
    "    try:\n",
    "        entry = dataset[idx]\n",
    "        question = entry[0]\n",
    "        answer = entry[1]\n",
    "        response, str_response, logits, start_pos, correct,\\\n",
    "                  hitemp_str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall =\\\n",
    "                                            answer_func(question, answer, tokenizer, temperature)\n",
    "        input_ids = tokenizer(question, return_tensors='pt').input_ids.to(device)\n",
    "    except:\n",
    "        continue\n",
    "    selfcheck_dict['question'].append(question)\n",
    "    selfcheck_dict['response'].append(response)\n",
    "    selfcheck_dict['str_response'].append(str_response)\n",
    "    selfcheck_dict['start_pos'].append(start_pos)\n",
    "    selfcheck_dict['correct'].append(correct)\n",
    "    selfcheck_dict['hitemp_str_responses'].append(hitemp_str_responses)\n",
    "    selfcheck_dict['selfcheck_scores_bert_overall'].append(selfcheck_scores_bert_overall)\n",
    "    selfcheck_dict['selfcheck_scores_bert_average'].append(selfcheck_scores_bert_average)\n",
    "    selfcheck_dict['selfcheck_ngram_overall'].append(selfcheck_ngram_overall)\n",
    "\n",
    "    selfcheck_arr_overall.append(1.0-selfcheck_scores_bert_overall[0]) #bert score flipped\n",
    "    selfcheck_arr_average.append(1.0-selfcheck_scores_bert_average[0]) #bert score flipped\n",
    "    selfcheck_ngram_average.append(1.0-np.exp(-selfcheck_ngram_overall[0]['doc_level']['avg_neg_logprob']))\n",
    "    correct_arr.append(int(correct))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1ff13-f25e-49b3-a14d-814d81f9689f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(selfcheck_arr_overall)\n",
    "#print(correct_arr)\n",
    "roc_score = roc_auc_score(correct_arr, selfcheck_arr_overall)\n",
    "print(f\"AUROC for self check overall: {roc_score}\")\n",
    "\n",
    "#print(selfcheck_arr_average)\n",
    "#print(correct_arr)\n",
    "roc_score = roc_auc_score(correct_arr, selfcheck_arr_average)\n",
    "print(f\"AUROC for self check average: {roc_score}\")\n",
    "\n",
    "roc_score = roc_auc_score(correct_arr, selfcheck_ngram_average)\n",
    "print(f\"AUROC for self check ngram: {roc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f64d92-b9a6-4772-ad75-685320e5259d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(f\"selfcheck_{model_name}_{dataset_name}_{gpu}.pickle\", \"wb\") as outfile:\n",
    "#         outfile.write(pickle.dumps(selfcheck_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e016206-3f46-420a-a94e-827fa28ceb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selfcheck_dict['hitemp_str_responses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac764a-6b59-4680-be92-7ceb0cbeb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfcheck_dict['hitemp_str_responses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95294a-bd6a-463a-b733-0486eb75927a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "22a0ee0a-c1f5-4450-9830-10a1383f28fe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hallucination",
   "language": "python",
   "name": "conda_hallucination"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
